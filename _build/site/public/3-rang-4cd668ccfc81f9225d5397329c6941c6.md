# 3. Rank of a matrix

## 3.1 Introduction
As we have mentioned on the first seminar, the main idea behind the notion of a matrix is to simultaneously keep track of various data we want to keep track of. Now we ask the following question: how to recognize if there is a relationship between data we are measuring?
:::{caution} Example
:icon: false
A market analyst wants to predict the price of a commodity based on the following parameters: supply $S$, demand $D$ and market price $P$. While analyzing past data, the analyst noticed an interesting pattern:
\begin{equation*} P = 2D - \frac{1}{2}S.\end{equation*}
Based on this information, does the analyst really need to measure all three parameters?
:::
## 3.2 Vectors
**Vector** is either a row-matrix or a column-matrix. Hence, a **row-vector** is just another name for a row-matrix, and **column-vector** is just another name for a column-matrix. Because vectors are just matrices with one row or one column, we know how to add vectors and we know how to multiply vectors by a scalar. 

Notice that a vector is determined by the number of its elements. For example, the vector $\begin{bmatrix} 1 & 2 & 3\end{bmatrix}$ has three elements. We will be refering to the elements of a vector as its **components**. Hence, the vector $\begin{bmatrix} 1 & 2 & 3\end{bmatrix}$ has three components. The set of all vectors with $n$ components is denoted by $\R^n.$

:::{tip} Definition
:icon:false
Let $a_1, \dots, a_k \in \R^n$ be vectors with $n$ components and let $c_1, \dots, c_k \in \R$ be real numbers. The vector
\begin{equation*} c_1a_1 + \dots + c_k a_k\end{equation*}
is called **a linear combination** of vectors $a_1, \dots, a_k.$
:::

:::{note} Problem 3.1
:icon:false
Let $a_1 = \begin{bmatrix} 1 & 1 & 0\end{bmatrix}, a_2 = \begin{bmatrix} 5 & -2 & 3 \end{bmatrix}, a_3 = \begin{bmatrix} -1 & 0 & 0 \end{bmatrix}.$ Find the linear combination of these vectors given by $v = -3a_1+2a_2-a_3.$
:::


:::{tip} Definition
:icon:false
:label: nezavisnost
The vectors $a_1, \dots, a_k$ are said to be **linearly independent** if 
\begin{equation*}c_1a_1 + c_2a_2 + \dots + c_k a_k = 0 \implies c_1 = c_2 = \dots = c_k = 0 \end{equation*}
Otherwise, we say that these vectors are **linearly dependent**.
:::
On the lectures, you have learnt that that if the vectors $a_1, \dots, a_k$ are linearly dependent, then one of those vectors can be represented as a linear combination of the others.

:::{error} Common mistake on the Midterm/Exam
:class:dropdown
In the sentence above, notice what is the defining property of linear dependence and what is the consequence of the definition. By [definition](#nezavisnost), the vectors are linearly dependent if there is a non-trivial linear combination of those vectors that is equal to the zero-vector $0$. So, that is the defining property of the linear dependence. The consequence of such a definition is that **if** the vectors are linearly dependent, **then** one of them is a linear combination of others. \
A lot of students confuse these two notions when they are questioned on it on the Midterms or the Exam, so keep in mind what is the definition and what is the consequence of the definition.
:::

:::{note} Problem 3.2
:icon:false
Check if the following vectors are linearly dependent or independent:
\begin{equation*} a_1 = \begin{bmatrix}  1 & 1 & 0 \end{bmatrix}, \quad a_2 = \begin{bmatrix} 1 & 2 & 0 \end{bmatrix}, \quad a_3 = \begin{bmatrix} 2 & 0 & 1 \end{bmatrix}. \end{equation*}
:::

## 3.3 Rank of a matrix
As we have seen, checking whether or not the given vectors are linearly dependent or independent using the definition can be quite messy - it boils down to solving systems of equations. So, we would like to come up with an easier method of checking the linear (in)dependence of vectors.

:::{tip} Definition
:icon:false
**The rank** of the matrix $A$ is the maximum number of its linearly independent columns. That number is denoted by $r(A).$
:::

:::{note} Problem 3.3
:icon:false
Find the rank of the matrix $A = \begin{bmatrix} 1 & 5 \\ 0 & 1 \end{bmatrix}.$
:::

:::{note} Problem 3.4
:icon: false
Find the rank of the matrix $A = \begin{bmatrix} 1 & 1 & -2 \\ 0 & 1 & -2 \\ 0 & 0 & 4\end{bmatrix}.$
:::

:::{note} Problem 3.5
:icon:false
Find the rank of the matrix $A = \begin{bmatrix} 1 & 1 & -2 \\ 0 & 1 & -2 \\ 0 & 0 & 0\end{bmatrix}.$
:::

:::{note} Problem 3.6
:icon:false
Find the rank of the matrix $A = \begin{bmatrix} 1 & 2 & 3 & 6 \\ 0 & 5 & 6 & 11 \\ 0 & 0 & 8 & 8 \\ 0 & 0 & 0 & 0\end{bmatrix}.$
:::

:::{note} Problem 3.7
:icon:false
Find the rank of the matrix $A = \begin{bmatrix}1 & 0 & 2 & 1 \\ 0 & 1 & -3 & 1 \\ 0 & 0 & 1 & 0 \end{bmatrix}.$
:::

:::{note} ‚öôÔ∏è Gauss-Jordan algorithm
:icon:false
1. Choose an element in the first column that you can use to cancel out all other elements in the first column
2. Interchange the corresponding rows so that the chosen element is in the position $(1,1)$
3. Cancel out the elements underneath the chosen element
4. Forget about the first row and first column
5. Repeat
:::

:::{note} Problem 3.8
:icon:false
Find the rank of the matrix $A = \begin{bmatrix} 2 & 1 & 1 & 0 \\ 1 & 2 & -1 & -3 \\ 0 & 1 & -1 & -3 \\ 1 & 3 & -2 & -5 \end{bmatrix}.$
:::

:::{caution} üí° 
:icon: false
The main idea behind the Gauss-Jordan algorithm is to reduce the original matrix to a matrix whose rank we can determine just by looking at it and counting non-zero rows.
:::

One of the reasons we introduced the notion of a rank of a matrix is to check linear (in)dependence of vectors. Here's how we do it:
1. Given the vectors, we make a matrix whose rows are equal to the entries of the given vectors
2. If the rank of that matrix is equal to the number of vectors we started with, the vectors are independent
3. Otherwise, they are dependent

:::{note} Problem 3.9
:icon:false
Determine whether or not the following vectors are linearly dependent or independent:
\begin{equation*} \begin{bmatrix} 2 & 1 & 1 \end{bmatrix}, \quad \begin{bmatrix} 1 & 3 & 2 \end{bmatrix}, \quad \begin{bmatrix}1 & 0 & 1 \end{bmatrix}. \end{equation*}
:::

:::{danger} Theorem
:icon:false
Let $A$ be a square matrix. Then
\begin{equation*}
\begin{split}
A \text{ is regular } &\iff \text{det}(A) \neq 0 \iff A \text{ has full rank} \\
A \text{ is singular} &\iff \text{det}(A) = 0 \iff A \text{ doesn't have full rank}
\end{split}
\end{equation*}
:::

:::{note} Problem 3.10
:icon:false
Let $A = \begin{bmatrix} 1 & 0 & 0 & 3 \\ 0 & 1 & -2 & 0 \\ -2 & 3 & -2 & 3 \\ 0 & -3 & 3 & 3 \end{bmatrix}.$ Is the matrix $A$ regular or singular?
:::

## 3.4 Inverse of a matrix (remastered)
As we have seen in the previous chapter, we can use the determinant to calculate the inverse of a regular matrix:
\begin{equation*} M_{ij} \rightarrow C \rightarrow A^\ast \rightarrow A^{-1} \end{equation*}
We will now learn how to use the Gauss-Jordan algorithm to find the inverse of a regular matrix in a much easier way.
:::{note} Problem 3.11 
:icon:false
Using the Gauss-Jordan algorithm, find the inverse of the matrix $A = \begin{bmatrix} 1 & 1 & 0 \\ 1 & 1 & 0 \\ 1 & 0 & 0 \end{bmatrix}.$
:::
